{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import argparse\n",
    "import re\n",
    "from nltk import FreqDist\n",
    "import pickle\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Embedding, RepeatVector, Dense, TimeDistributed, Flatten\n",
    "from keras import initializers\n",
    "from keras import optimizers\n",
    "from keras.utils import get_file\n",
    "from keras.models import model_from_json\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(text_file):\n",
    "    text_raw = open(text_file, 'r')\n",
    "    text=[]\n",
    "    for line in text_raw:\n",
    "        text.append(re.compile('\\w+').findall(line))\n",
    "    text_raw.close()\n",
    "    print(\"Input text size:\"+str(len(text)))\n",
    "    \n",
    "    for sentence in text:\n",
    "        sentence.append(\"<EOS>\")        \n",
    "    \n",
    "    word_vectors_X = KeyedVectors.load_word2vec_format('xWordVectors.txt', binary=True)\n",
    "    word_vectors_Y = KeyedVectors.load_word2vec_format('yWordVectors.txt', binary=True)\n",
    "        \n",
    "    print(\"Vocabulary loaded\")  \n",
    "          \n",
    "    #LSTM NETWORK MODEL\n",
    "    # load json and create model\n",
    "    json_file = open('model.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(\"model.h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "                   \n",
    "    #replace words with vocabulary vector (if not found, word will be replaced with \"UNK\")\n",
    "    layer_cell_num=loaded_model.layers[0].input_shape[2]\n",
    "    for i, sentence in enumerate(text):\n",
    "        for j, word in enumerate(sentence):\n",
    "            if word in word_vectors_X.vocab:\n",
    "                text[i][j]=word_vectors_X.word_vec(word)\n",
    "            else:\n",
    "                text[i][j]=np.ones(layer_cell_num)\n",
    "                \n",
    "    print(\"Words are replaced by vectors in vocabulary\")            \n",
    "    \n",
    "    #flip input sentences\n",
    "    for sentence in text:\n",
    "        sentence.reverse() \n",
    "    \n",
    "    #padding and triming\n",
    "    X_max_len=loaded_model.layers[0].input_shape[1]\n",
    "    for sentence in text:\n",
    "        dif = X_max_len - len(sentence)\n",
    "        if dif<0:\n",
    "            del sentence[(X_max_len-1):]\n",
    "            sentence.append(\"<EOS>\")\n",
    "        while dif>0 :\n",
    "            sentence.append(np.zeros(layer_cell_num))\n",
    "            dif=dif-1  \n",
    "            \n",
    "    \n",
    "    preds = loaded_model.predict(text)\n",
    "    \n",
    "    for i, sentence in enumerate(preds):\n",
    "        for j, word in enumerate(sentence):\n",
    "                print(word_vectors_Y.similar_by_vector(preds[i][j], topn=1)) #TODO print to string\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#learn\n",
    "#-creates and traines LSTM neural nework of layer_cell_num layers, using X and Y datasets\n",
    "def learning(X_file, Y_file, vocab, layer_cell_num):\n",
    "    X_raw = open(X_file, 'r')\n",
    "    Y_raw = open(Y_file, 'r')\n",
    "    \n",
    "    #parsing input and output files\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    for line in X_raw:\n",
    "        X.append((re.compile('\\w+').findall(line)))\n",
    "    X_raw.close()\n",
    "    print(\"Input sample size:\"+str(len(X)))\n",
    "    \n",
    "    for line in Y_raw:\n",
    "        Y.append((re.compile('\\w+').findall(line)))\n",
    "    Y_raw.close()\n",
    "    print(\"Output sample size (has to match input):\"+str(len(Y)))\n",
    "    \n",
    "    #due to fixed size input to keras RNN (this can be overriden, but is irrelevant in this case), padding is necessary\n",
    "    X_max_len=max(len(sublist) for sublist in X)+1\n",
    "    Y_max_len=max(len(sublist) for sublist in Y)+1\n",
    "    \n",
    "    for sentence in X:\n",
    "        sentence.append(\"<EOS>\")\n",
    "    for sentence in Y:\n",
    "        sentence.append(\"<EOS>\")\n",
    "        \n",
    "    \n",
    "    if vocab>0:\n",
    "        print(\"Creating vocabulary\")\n",
    "        model_X = word2vec.Word2Vec(X, size=layer_cell_num, max_vocab_size =2*vocab)\n",
    "        model_Y = word2vec.Word2Vec(Y, size=layer_cell_num, max_vocab_size =vocab)\n",
    "        word_vectors_X = model_X.wv\n",
    "        word_vectors_Y = model_Y.wv\n",
    "        word_vectors_X.save_word2vec_format('xWordVectors.txt', binary=True)\n",
    "        word_vectors_Y.save_word2vec_format('yWordVectors.txt', binary=True)\n",
    "        del model_X\n",
    "        del model_Y\n",
    "    else:\n",
    "        word_vectors_X = KeyedVectors.load_word2vec_format('xWordVectors.txt', binary=True)\n",
    "        word_vectors_Y = KeyedVectors.load_word2vec_format('yWordVectors.txt', binary=True)\n",
    "        \n",
    "    print(\"Vocabulary created\")\n",
    "            \n",
    "    #replace words with vocabulary vector (if not found, word will be replaced with \"UNK\")\n",
    "    for i, sentence in enumerate(X):\n",
    "        for j, word in enumerate(sentence):\n",
    "            if word in word_vectors_X.vocab:\n",
    "                X[i][j]=word_vectors_X.word_vec(word)\n",
    "            else:\n",
    "                X[i][j]=np.ones(layer_cell_num)\n",
    "    for i, sentence in enumerate(Y):\n",
    "        for j, word in enumerate(sentence):\n",
    "            if word in word_vectors_Y.vocab:\n",
    "                Y[i][j]=word_vectors_Y.word_vec(word)\n",
    "            else:\n",
    "                Y[i][j]=np.ones(layer_cell_num)\n",
    "                \n",
    "    print(\"Words are replaced by vectors in vocabulary\")            \n",
    "    \n",
    "    #flip input sentences\n",
    "    for sentence in X:\n",
    "        sentence.reverse()   \n",
    "    \n",
    "    #add padding with zeros to achive a fixed size input\n",
    "    \n",
    "    for sentence in X:\n",
    "        dif = X_max_len - len(sentence)\n",
    "        while dif>0 :\n",
    "            sentence.append(np.zeros(layer_cell_num))\n",
    "            dif=dif-1\n",
    "            \n",
    "    for sentence in Y:\n",
    "        dif = Y_max_len - len(sentence)\n",
    "        while dif>0 :\n",
    "            sentence.append(np.zeros(layer_cell_num))\n",
    "            dif=dif-1        \n",
    "        \n",
    "    #LSTM NETWORK MODEL\n",
    "    #encoder network\n",
    "    print(\"Creating model, input shape: \" + str(X_max_len) + \", \"+ str(layer_cell_num))\n",
    "    model=Sequential()\n",
    "    \n",
    "    model.add(LSTM(layer_cell_num, input_shape=(X_max_len, layer_cell_num), activation='sigmoid', recurrent_activation='sigmoid', kernel_initializer=initializers.RandomUniform(minval=-0.08, maxval=0.08, seed=None), recurrent_initializer=initializers.RandomUniform(minval=-0.08, maxval=0.08, seed=None), return_sequences=True))\n",
    "    model.add(LSTM(layer_cell_num, activation='sigmoid', recurrent_activation='sigmoid', kernel_initializer=initializers.RandomUniform(minval=-0.08, maxval=0.08, seed=None), recurrent_initializer=initializers.RandomUniform(minval=-0.08, maxval=0.08, seed=None), return_sequences=True))\n",
    "    model.add(LSTM(layer_cell_num, activation='sigmoid', recurrent_activation='sigmoid', kernel_initializer=initializers.RandomUniform(minval=-0.08, maxval=0.08, seed=None), recurrent_initializer=initializers.RandomUniform(minval=-0.08, maxval=0.08, seed=None), return_sequences=True))\n",
    "    model.add(LSTM(layer_cell_num, activation='sigmoid', recurrent_activation='sigmoid', kernel_initializer=initializers.RandomUniform(minval=-0.08, maxval=0.08, seed=None), recurrent_initializer=initializers.RandomUniform(minval=-0.08, maxval=0.08, seed=None)))\n",
    "    model.add(RepeatVector(Y_max_len))\n",
    "              \n",
    "   \n",
    "    model.add(LSTM(layer_cell_num, activation='sigmoid', recurrent_activation='sigmoid', kernel_initializer=initializers.RandomUniform(minval=-0.08, maxval=0.08, seed=None), recurrent_initializer=initializers.RandomUniform(minval=-0.08, maxval=0.08, seed=None), return_sequences=True))\n",
    "    model.add(LSTM(layer_cell_num, activation='sigmoid', recurrent_activation='sigmoid', kernel_initializer=initializers.RandomUniform(minval=-0.08, maxval=0.08, seed=None), recurrent_initializer=initializers.RandomUniform(minval=-0.08, maxval=0.08, seed=None), return_sequences=True))\n",
    "    model.add(LSTM(layer_cell_num, activation='sigmoid', recurrent_activation='sigmoid', kernel_initializer=initializers.RandomUniform(minval=-0.08, maxval=0.08, seed=None), recurrent_initializer=initializers.RandomUniform(minval=-0.08, maxval=0.08, seed=None), return_sequences=True))\n",
    "    model.add(LSTM(layer_cell_num, activation='sigmoid', recurrent_activation='sigmoid', kernel_initializer=initializers.RandomUniform(minval=-0.08, maxval=0.08, seed=None), recurrent_initializer=initializers.RandomUniform(minval=-0.08, maxval=0.08, seed=None), return_sequences=True))\n",
    "    print(model.summary())\n",
    "\n",
    "    sgd = optimizers.SGD(lr=0.7, clipnorm=5)\n",
    "    model.compile(loss='mean_squared_error', optimizer=sgd)\n",
    "    \n",
    "    X_array=np.asarray(X)\n",
    "    \n",
    "    print(\"Training and testing model\")\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33)\n",
    "    \n",
    "    model.fit(X_train, Y_train, epochs=7, batch_size=128)\n",
    "    \n",
    "    model.evaluate(X_test, Y_test)\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_model(model):\n",
    "\n",
    "    # serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(\"model.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(\"model.h5\")\n",
    "    print(\"Saved model to disk\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
