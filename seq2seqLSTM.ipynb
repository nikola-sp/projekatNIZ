{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import argparse\n",
    "import re\n",
    "from nltk import FreqDist\n",
    "import pickle\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Embedding, RepeatVector, Dense, TimeDistributed, Flatten\n",
    "from keras import initializers\n",
    "from keras import optimizers\n",
    "from keras.utils import get_file\n",
    "from keras.models import model_from_json\n",
    "\n",
    "from sklearn import model_selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create_vocab\n",
    "#-creates a vocabulary of vocab_size most common words from source if vocab_size>0, without words longer than max-len\n",
    "def create_vocab(source, vocab_size, name):\n",
    "    print(\"Creating vocabulary\")\n",
    "    flat_source = [val for sublist in source for val in sublist]\n",
    "    fdist=FreqDist(flat_source)\n",
    "    most_common = fdist.most_common(vocab_size-1)\n",
    "    \n",
    "    dest=[]\n",
    "    for word, apperances in most_common:\n",
    "        dest.append(word)\n",
    "    dest.append('UNK')\n",
    "    dest.insert(0, '<PAD>')   #0 is used for simple masking\n",
    "    \n",
    "    print(\"Saving vocabulary\")\n",
    "    with open(name, \"wb\") as fp:\n",
    "        pickle.dump(dest, fp)\n",
    "        \n",
    "    return dest\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def translate(text, model):\n",
    "    print('not implemented')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#learn\n",
    "#-creates and traines LSTM neural nework of layer_cell_num layers, using X and Y datasets\n",
    "def learning(X_file, Y_file, vocab, layer_cell_num):\n",
    "    X_raw = open(X_file, 'r')\n",
    "    Y_raw = open(Y_file, 'r')\n",
    "    \n",
    "    #parsing input and output files\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    for line in X_raw:\n",
    "        X.append(re.compile('\\w+').findall(line))\n",
    "    X_raw.close()\n",
    "    print(\"Input sample size:\"+str(len(X)))\n",
    "    \n",
    "    for line in Y_raw:\n",
    "        Y.append(re.compile('\\w+').findall(line))\n",
    "    Y_raw.close()\n",
    "    print(\"Output sample size (has to match input):\"+str(len(Y)))\n",
    "    \n",
    "    #due to fixed size input to keras RNN (this can be overriden, but is irrelevant in this case), padding is necessary\n",
    "    X_max_len=max(len(sublist) for sublist in X)\n",
    "    Y_max_len=max(len(sublist) for sublist in Y)\n",
    "    \n",
    "    print(\"Max sentence size:\"+str(X_max_len)+\" \"+str(Y_max_len))\n",
    "    \n",
    "    #create or read a vocabulary\n",
    "    X_vocab=[]\n",
    "    Y_vocab=[]\n",
    "    if vocab>0:\n",
    "        X_vocab = create_vocab(X, 2*vocab, 'vocabularySrc.txt') #160000\n",
    "        Y_vocab = create_vocab(Y, vocab,'vocabularyDest.txt')   #80000\n",
    "    else:\n",
    "        print(\"Opening vocabulary\")\n",
    "        with open(\"vocabularySrc.txt\", \"rb\") as fp:\n",
    "            X_vocab = pickle.load(fp)\n",
    "        with open(\"vocabularyDest.txt\", \"rb\") as fp:\n",
    "            Y_vocab = pickle.load(fp)\n",
    "            \n",
    "    #replace words with vocabulary location (if not found, word will be replaced with \"UNK\")\n",
    "    for i, sentence in enumerate(X):\n",
    "        for j, word in enumerate(sentence):\n",
    "            if word in X_vocab:\n",
    "                X[i][j]=X_vocab.index(word)\n",
    "            else:\n",
    "                X[i][j]=X_vocab.index('UNK')\n",
    "    for i, sentence in enumerate(Y):\n",
    "        for j, word in enumerate(sentence):\n",
    "            if word in Y_vocab:\n",
    "                Y[i][j]=Y_vocab.index(word)\n",
    "            else:\n",
    "                Y[i][j]=Y_vocab.index('UNK')\n",
    "    print(\"Words are replaced by numbers in vocabulary\")            \n",
    "    \n",
    "    #flip input sentences\n",
    "    for sentence in X:\n",
    "        rsent = list(reversed(sentence))\n",
    "        sentence = rsent\n",
    "        \n",
    "    #add padding with zeros to achive a fixed size input\n",
    "    X = pad_sequences(X, maxlen=X_max_len, dtype='int32', value=0)\n",
    "    Y = pad_sequences(Y, maxlen=Y_max_len, dtype='int32', value=0)\n",
    "        \n",
    "        \n",
    "    #LSTM NETWORK MODEL\n",
    "    #encoder network\n",
    "    print(\"Creating model\")\n",
    "    model=Sequential()\n",
    "    \n",
    "    model.add(Embedding(2*vocab, layer_cell_num, input_length = X_max_len, mask_zero = True))  #embedding of words in 1000 D space and pading elimination\n",
    "    model.add(LSTM(layer_cell_num, activation='sigmoid', recurrent_activation='sigmoid', kernel_initializer=initializers.RandomUniform(minval=-0.08, maxval=0.08, seed=None), recurrent_initializer=initializers.RandomUniform(minval=-0.08, maxval=0.08, seed=None), return_sequences=True))\n",
    "    model.add(LSTM(layer_cell_num, activation='sigmoid', recurrent_activation='sigmoid', kernel_initializer=initializers.RandomUniform(minval=-0.08, maxval=0.08, seed=None), recurrent_initializer=initializers.RandomUniform(minval=-0.08, maxval=0.08, seed=None), return_sequences=True))\n",
    "    model.add(LSTM(layer_cell_num, activation='sigmoid', recurrent_activation='sigmoid', kernel_initializer=initializers.RandomUniform(minval=-0.08, maxval=0.08, seed=None), recurrent_initializer=initializers.RandomUniform(minval=-0.08, maxval=0.08, seed=None), return_sequences=True))\n",
    "    model.add(LSTM(layer_cell_num, activation='sigmoid', recurrent_activation='sigmoid', kernel_initializer=initializers.RandomUniform(minval=-0.08, maxval=0.08, seed=None), recurrent_initializer=initializers.RandomUniform(minval=-0.08, maxval=0.08, seed=None)))\n",
    "    model.add(RepeatVector(Y_max_len))\n",
    "              \n",
    "   \n",
    "    model.add(LSTM(layer_cell_num, activation='sigmoid', recurrent_activation='sigmoid', kernel_initializer=initializers.RandomUniform(minval=-0.08, maxval=0.08, seed=None), recurrent_initializer=initializers.RandomUniform(minval=-0.08, maxval=0.08, seed=None), return_sequences=True))\n",
    "    model.add(LSTM(layer_cell_num, activation='sigmoid', recurrent_activation='sigmoid', kernel_initializer=initializers.RandomUniform(minval=-0.08, maxval=0.08, seed=None), recurrent_initializer=initializers.RandomUniform(minval=-0.08, maxval=0.08, seed=None), return_sequences=True))\n",
    "    model.add(LSTM(layer_cell_num, activation='sigmoid', recurrent_activation='sigmoid', kernel_initializer=initializers.RandomUniform(minval=-0.08, maxval=0.08, seed=None), recurrent_initializer=initializers.RandomUniform(minval=-0.08, maxval=0.08, seed=None), return_sequences=True))\n",
    "    model.add(LSTM(layer_cell_num, activation='sigmoid', recurrent_activation='sigmoid', kernel_initializer=initializers.RandomUniform(minval=-0.08, maxval=0.08, seed=None), recurrent_initializer=initializers.RandomUniform(minval=-0.08, maxval=0.08, seed=None), return_sequences=True)) \n",
    "    model.add(Flatten())\n",
    "    model.add(TimeDistributed(Dense(vocab)))\n",
    "    print(model.summary())\n",
    "\n",
    "    sgd = optimizers.SGD(lr=0.7, clipnorm=5)\n",
    "    model.compile(loss='mean_squared_error', optimizer=sgd)\n",
    "    \n",
    "    print(\"Training and testing model\")\n",
    "    X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y, test_size=0.33)\n",
    "    \n",
    "    model.fit(X_train, Y_train, epochs=7, batch_size=128)\n",
    "    \n",
    "    model.evaluate(X_test, Y_test)\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_model(model):\n",
    "\n",
    "    # serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(\"model.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(\"model.h5\")\n",
    "    print(\"Saved model to disk\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
